{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WOw8yMd1VlnD"},"source":["# Data Preprocessing Workbook w/Notes"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NvUGC8QQV6bV"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"wfFEXZC0WS-V"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/fy/sdkjccl94_n8cnq_k1rbmzp40000gn/T/ipykernel_58458/328324305.py:3: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fhYaZ-ENV_c5"},"source":["## Importing the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{},"colab_type":"code","id":"aqHTg9bxWT_u"},"outputs":[],"source":["dataset = pd.read_csv('Data.csv')\n","X = dataset.iloc[:, :-1].values # iloc[rows, columns] : means all (iloc is integer location for rows and columns)\n","y = dataset.iloc[:, -1].values  # Assumes (as will often be the case) Your dependent variable is the last column (this says just take the last (-1) column)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X:  [['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 nan]\n"," ['France' 35.0 58000.0]\n"," ['Spain' nan 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n","y:  ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"]}],"source":["print('X: ', X)\n","print('y: ', y)"]},{"cell_type":"markdown","metadata":{},"source":["#### Both X and Y are matrices. And are the independent and dependent variables respectively.\n","#### You want to avoid having missing or nan values in your data set. But to combat it you:\n","- Run an Imputer substitution using package. \n","- Include all of the numeric values into the transform function (or all the replacement string values if you were to need to fill null values)\n","- "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X:  [['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 63777.77777777778]\n"," ['France' 35.0 58000.0]\n"," ['Spain' 38.77777777777778 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}],"source":["from sklearn.impute import SimpleImputer\n","missing_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')   # Using average of all present values in dataset\n","missing_imputer.fit(X[:, 1:3]) # Here you ask for dataset X, give all columns with numeric values (2nd and 3rd columns, or indexes 1:3)\n","X[:, 1:3] = missing_imputer.transform(X[:, 1:3]) # Here you again are saying replace these columns from matrix/dataset X no with the 'mocked' averaged values\n","print('X: ', X)"]},{"cell_type":"markdown","metadata":{},"source":["#### Encoding Categorical (String) Data\n","##### One Hot Encoding\n","- Simplest method is take your column of string values, count the number of different values / categories . Then create that same number of columns out of that 1 column. So a column with categories of 3 countries, 3 new vector columns get created. i.e. $|1,0,0|, |0,1,0|, |0,0,1|$\n","- Below you'll see how we implement this. Basically in our example case we have 3 country values for our country feature (column). _One Hot Encooding_ Simply creates $n$ columns based upon $n$ categories you want to encode. So instead of 1 column with 3 country values, we will transform to have 3 columns with a numerical boolean representation for the country (1 or 0)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X:  [[1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 35.0 58000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}],"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') # Here you are saying that you want to encode the first column (index 0) of the dataset X\n","X = np.array(ct.fit_transform(X))\n","print('X: ', X)"]},{"cell_type":"markdown","metadata":{},"source":["### Encode Dependent Variable (Labels)\n","- In our example we have yes/no labels. \n","- Convert them to binary 1/0 (LabelEncoder class from scikit learn)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["y:  [0 1 0 0 1 1 0 1 0 1]\n"]}],"source":["# Do the same 'vectorization' (string to number but not ordinal) for Y (dependent variable / 'Label')\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","print('y: ', y)"]},{"cell_type":"markdown","metadata":{},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"markdown","metadata":{},"source":["#### Apply feature scaling _after_ splitting training and test sets\n","- Reason you want to apply feature scaling after splitting your data is that the SD (used in feature scaling) would be calculated off the mean of all the data points _if_ you included the test data (if you feature scaled _before_ splitting). This would mean you are bleeding information from the test set into the training set, and they are _supposed to be independent_ and un-informed of each other.\n","- Scikit Learn provides the __train_test_split__ function for doing this, implementation below:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)  # 'test_size' is the percentage of the dataset that will be used for testing (20% here)\n","                                                 # 'random_state' is the seed for the random number generator. Any integer will produce reproducible function results. https://scikit-learn.org/stable/glossary.html#term-random_state "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train:  [[0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 35.0 58000.0]]\n"]}],"source":["print('X_train: ', X_train)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_test:  [[0.0 1.0 0.0 30.0 54000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}],"source":["print('X_test: ', X_test)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["y_train:  [0 1 0 0 1 1 0 1]\n"]}],"source":["print('y_train: ', y_train)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["y_test:  [0 1]\n"]}],"source":["print('y_test: ', y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Scaling\n","- To recap we scale features to prevent any features in initial dataset from dominating other features in the set. When doing _proportionate analysis_ you need to ensure your feature values are not disproportionate. _(This isn't always neccessary)._\n","- [Formula for the basic 2 approaches](../../../../notes/2-DataPreprocessing.md/#feature-scaling) we've covered *Standardisation* and *Normalisation*\n","- Our instructor posits that _Standardisation_ will work in any type of Distribution, whereas _Normalisation_ typically only works well with a _normal distribution_. For that reason we'll be foccussing on using _Standardisation_\n","- Also important to note that when you encode categorical data like we did for the country names (by creating new columns and placing a 1 or 0) you _do not_ (need) to feature scale those dummy variable. "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train:  [[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n"," [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n"," [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n"," [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n"," [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n"," [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n"," [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n"," [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"]}],"source":["from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","\n","\n","X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])   # Here you are saying that you want to scale all rows (1st colon) of all columns \n","                                                    #from the 4th column (index 3) to the end of the dataset X_train\n","print('X_train: ', X_train)"]},{"cell_type":"markdown","metadata":{},"source":["#### [Scikit-learn's StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) class provides methods for _standardizing_ features. The [fit_transform() method](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform) is used to first *fit* the matrix of features to get the _mean_, and then transform the data into the *standardized* form."]},{"cell_type":"markdown","metadata":{},"source":["#### The sc object will set and store internal state (the statistics of the above running of the fit_transform method). For this reason, we want to *reuse* it on the test set data, and this makes sense, we wouldn't want to standardize one portion of our observational data and not do the same to the other. "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_test:  [[0.0 1.0 0.0 -6.6987360602674375 -5.528988873522207]\n"," [1.0 0.0 0.0 -6.677304402554669 -5.528988865372772]]\n"]}],"source":["\n","X_test[:, 3:] = sc.transform(X_test[:, 3:]) # Here you are saying that you want to scale all rows (1st colon) of all columns \n","                                            #from the 4th column (index 3) to the end of the dataset X_test\n","print('X_test: ', X_test)"]},{"cell_type":"markdown","metadata":{},"source":["#### Now that we are done scaling our features. Run it"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOD2/gZgY69JdiiGJVNfu7s","collapsed_sections":[],"name":"data_preprocessing_template.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
