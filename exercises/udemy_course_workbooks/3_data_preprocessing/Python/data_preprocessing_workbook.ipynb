{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WOw8yMd1VlnD"},"source":["# Data Preprocessing Workbook w/Notes"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NvUGC8QQV6bV"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"wfFEXZC0WS-V"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/fy/sdkjccl94_n8cnq_k1rbmzp40000gn/T/ipykernel_9305/328324305.py:3: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fhYaZ-ENV_c5"},"source":["## Importing the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{},"colab_type":"code","id":"aqHTg9bxWT_u"},"outputs":[],"source":["dataset = pd.read_csv('Data.csv')\n","X = dataset.iloc[:, :-1].values # iloc[rows, columns] : means all (iloc is integer location for rows and columns)\n","y = dataset.iloc[:, -1].values  # Assumes (as will often be the case) Your dependent variable is the last column (this says just take the last (-1) column)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X:  [['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 nan]\n"," ['France' 35.0 58000.0]\n"," ['Spain' nan 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n","y:  ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"]}],"source":["print('X: ', X)\n","print('y: ', y)"]},{"cell_type":"markdown","metadata":{},"source":["#### Both X and Y are matrices. And are the independent and dependent variables respectively.\n","#### You want to avoid having missing or nan values in your data set. But to combat it you:\n","- Run an Imputer substitution using package. \n","- Include all of the numeric values into the transform function (or all the replacement string values if you were to need to fill null values)\n","- "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X:  [['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 63777.77777777778]\n"," ['France' 35.0 58000.0]\n"," ['Spain' 38.77777777777778 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}],"source":["from sklearn.impute import SimpleImputer\n","missing_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')   # Using average of all present values in dataset\n","missing_imputer.fit(X[:, 1:3]) # Here you ask for dataset X, give all columns with numeric values (2nd and 3rd columns, or indexes 1:3)\n","X[:, 1:3] = missing_imputer.transform(X[:, 1:3]) # Here you again are saying replace these columns from matrix/dataset X no with the 'mocked' averaged values\n","print('X: ', X)"]},{"cell_type":"markdown","metadata":{},"source":["#### Encoding Categorical (String) Data\n","##### One Hot Encoding\n","- Simplest method is take your column of string values, count the number of different values / categories . Then create that same number of columns out of that 1 column. So a column with categories of 3 countries, 3 new vector columns get created. i.e. $|1,0,0|, |0,1,0|, |0,0,1|$\n","- Below you'll see how we implement this. Basically in our example case we have 3 country values for our country feature (column). _One Hot Encooding_ Simply creates $n$ columns based upon $n$ categories you want to encode. So instead of 1 column with 3 country values, we will transform to have 3 columns with a numerical boolean representation for the country (1 or 0)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X:  [[1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 35.0 58000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}],"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') # Here you are saying that you want to encode the first column (index 0) of the dataset X\n","X = np.array(ct.fit_transform(X))\n","print('X: ', X)"]},{"cell_type":"markdown","metadata":{},"source":["### Encode Dependent Variable (Labels)\n","- In our example we have yes/no labels. \n","- Convert them to binary 1/0 (LabelEncoder class from scikit learn)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["y:  [0 1 0 0 1 1 0 1 0 1]\n"]}],"source":["# Do the same 'vectorization' (string to number but not ordinal) for Y (dependent variable / 'Label')\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","print('y: ', y)"]},{"cell_type":"markdown","metadata":{},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"markdown","metadata":{},"source":["#### Apply feature scaling _after_ splitting training and test sets\n","- Reason you want to apply feature scaling after splitting your data is that the SD (used in feature scaling) would be calculated off the mean of all the data points _if_ you included the test data (if you feature scaled _before_ splitting). This would mean you are bleeding information from the test set into the training set, and they are _supposed to be independent_ and un-informed of each other.\n","- Scikit Learn provides the __train_test_split__ function for doing this, implementation below:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train:  [[0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 35.0 58000.0]]\n"]}],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)  # 'test_size' is the percentage of the dataset that will be used for testing (20% here)\n","print('X_train: ', X_train)                                                          # 'random_state' is the seed for the random number generator. Any integer will produce reproducible function results. https://scikit-learn.org/stable/glossary.html#term-random_state "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOD2/gZgY69JdiiGJVNfu7s","collapsed_sections":[],"name":"data_preprocessing_template.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
